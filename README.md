# Members: *Pablo Campos, Ignacio Sebastia, Diego Gastaldi.*

** ## Step 1: **
### Create a repository on GitHub and connect it with peers.
## Step 2: 
### Find the right images to interpret sign language.
** ## Step 3: **
### Extract those images using web scraping "imagenes.ipynb". In this case, we have done it through a request and Beautiful Soup. Once the images have been obtained, we have put them in a folder "project_imagenes" and downloaded it to Drive. Later, we have uploaded it to Git. We have had some problems trying to insert too many images.
** ## Step 4: **
### We have used Roboflow to classify and create a model with the previous images.
** ## Step 5: **
### Using Hugging Face, we take a model of it and apply it to our data.
