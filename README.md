Members: Pablo Campos, Ignacio Sebastia, Diego Gastaldi.

Step 1: Create a repository on github and connect it with peers
Step 2: Find the right images to interpret sign language
Step 3: Extract those images using web scraping "imagenes.ipynb", in this case we have done it through a request and beautiful soap, once the images have been obtained we have put them in a folder "project_imagenes" and we have downloaded it to drive, later we have uploaded it to git
Step 4: We have used roboflow to classify and create a model with the previous images
Step 5: Using hugging face, we take a model of it and apply it to our data.
