# H1 Members: Pablo Campos, Ignacio Sebastia, Diego Gastaldi.

# H2 Step 1: Create a repository on github and connect it with peers.
# H2 Step 2: Find the right images to interpret sign language.
# H2 Step 3: Extract those images using web scraping "imagenes.ipynb", in this case we have done it through a request and beautiful soap, once the images have been obtained we have put them in a folder "project_imagenes" and we have downloaded it to drive, later we have uploaded it to git.
# H2 Step 4: We have used roboflow to classify and create a model with the previous images.
# H2 Step 5: Using hugging face, we take a model of it and apply it to our data.
